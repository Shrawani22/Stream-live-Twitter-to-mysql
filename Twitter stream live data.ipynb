{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pymysql\n",
    "import json\n",
    "from dateutil import parser\n",
    "import time\n",
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = '*'\n",
    "consumer_secret= '*'\n",
    "access_token= '*-*'\n",
    "access_token_secret = '*'\n",
    "password = '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(username, created_at, tweet, retweet_count, place , location):\n",
    "\t\"\"\"\n",
    "\tconnect to MySQL database and insert twitter data\n",
    "\t\"\"\"\n",
    "\ttry:\n",
    "\t\tcon = mysql.connector.connect(host = 'localhost',\n",
    "\t\tdatabase = 'twitterdb', user='root', password = password, charset = 'utf8')\n",
    "\n",
    "\t\tif con.is_connected():\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tInsert twitter data\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tcursor = con.cursor()\n",
    "\t\t\t# twitter, golf\n",
    "\t\t\tquery = \"INSERT INTO twitterdata (username, created_at, tweet, retweet_count, place, location) VALUES (%s, %s, %s, %s, %s, %s)\"\n",
    "\t\t\tcursor.execute(query, (username, created_at, tweet, retweet_count, location, place))\n",
    "\t\t\tcon.commit()\n",
    "            \n",
    "\texcept Error as e: \n",
    "\t\tprint(e)   \n",
    "\tcursor.close()\n",
    "\tcon.close()\n",
    "\n",
    "\treturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweepy class to access Twitter API\n",
    "class Streamlistener(tweepy.StreamListener):\n",
    "\t\n",
    "\n",
    "\tdef on_connect(self):\n",
    "\t\tprint(\"You are connected to the Twitter API\")\n",
    "\n",
    "\n",
    "\tdef on_error(self):\n",
    "\t\tif status_code != 200:\n",
    "\t\t\tprint(\"error found\")\n",
    "\t\t\t# returning false disconnects the stream\n",
    "\t\t\treturn False\n",
    "\n",
    "\t\"\"\"\n",
    "\tThis method reads in tweet data as Json\n",
    "\tand extracts the data we want.\n",
    "\t\"\"\"\n",
    "\tdef on_data(self,data):\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\traw_data = json.loads(data)\n",
    "\n",
    "\t\t\tif 'text' in raw_data:\n",
    "\t\t\t\t \n",
    "\t\t\t\tusername = raw_data['user']['screen_name']\n",
    "\t\t\t\tcreated_at = parser.parse(raw_data['created_at'])\n",
    "\t\t\t\ttweet = raw_data['text']\n",
    "\t\t\t\tretweet_count = raw_data['retweet_count']\n",
    "\n",
    "\t\t\t\tif raw_data['place'] is not None:\n",
    "\t\t\t\t\tplace = raw_data['place']['country']\n",
    "\t\t\t\t\tprint(place)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tplace = None\n",
    "\t\t\t\t\n",
    "\n",
    "\t\t\t\tlocation = raw_data['user']['location']\n",
    "\n",
    "\t\t\t\t#insert data just collected into MySQL database\n",
    "\t\t\t\tconnect(username, created_at, tweet, retweet_count, place, location)\n",
    "\t\t\t\tprint(\"Tweet colleted at: {} \".format(str(created_at)))\n",
    "\t\texcept Error as e:\n",
    "\t\t\tprint(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__== '__main__':\n",
    "\n",
    "\t# # #Allow user input\n",
    "\t# track = []\n",
    "\t# while True:\n",
    "\n",
    "\t# \tinput1  = input(\"what do you want to collect tweets on?: \")\n",
    "\t# \ttrack.append(input1)\n",
    "\n",
    "\t# \tinput2 = input(\"Do you wish to enter another word? y/n \")\n",
    "\t# \tif input2 == 'n' or input2 == 'N':\n",
    "\t# \t\tbreak\n",
    "\t\n",
    "\t# print(\"You want to search for {}\".format(track))\n",
    "\t# print(\"Initialising Connection to Twitter API....\")\n",
    "\t# time.sleep(2)\n",
    "\n",
    "\t# authentification so we can access twitter\n",
    "\tauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "\tauth.set_access_token(access_token, access_token_secret)\n",
    "\tapi =tweepy.API(auth, wait_on_rate_limit=True)\n",
    "\n",
    "\t# create instance of Streamlistener\n",
    "\tlistener = Streamlistener(api = api)\n",
    "\tstream = tweepy.Stream(auth, listener = listener)\n",
    "\n",
    "\ttrack = ['data science', 'machine learning', 'Artificial intelligence', 'Deep Learning', 'ml','dl']\n",
    "\t#track = ['nba', 'cavs', 'celtics', 'basketball']\n",
    "\t# choose what we want to filter by\n",
    "\tstream.filter(track = track, languages = ['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterdb = pymysql.connect(\"localhost\", \"root\", \"ck1234\", \"twitterdb\")\n",
    "query = \"select * from twitterdata\"\n",
    "data = pd.read_sql(query, twitterdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2205, 7)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-30 19:41:10</td>\n",
       "      <td>joker_fantasy_m</td>\n",
       "      <td>RT @Jav_grandpa: [DASD-669] An Erotically Brai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-05-30 19:41:10</td>\n",
       "      <td>CyberSecurityN8</td>\n",
       "      <td>RT @gp_pulipaka: Best #Books That Will Teach Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-05-30 19:41:12</td>\n",
       "      <td>sectest9</td>\n",
       "      <td>RT @gp_pulipaka: Best #Books That Will Teach Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-05-30 19:41:13</td>\n",
       "      <td>ishancarter</td>\n",
       "      <td>RT @CaptMarkKelly: I’m running for Senate to b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-05-30 19:41:13</td>\n",
       "      <td>TGNewsfall</td>\n",
       "      <td>#Microsoft is reducing dozens of MSN records p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at         username  \\\n",
       "0  2020-05-30 19:41:10  joker_fantasy_m   \n",
       "1  2020-05-30 19:41:10  CyberSecurityN8   \n",
       "2  2020-05-30 19:41:12         sectest9   \n",
       "3  2020-05-30 19:41:13      ishancarter   \n",
       "4  2020-05-30 19:41:13       TGNewsfall   \n",
       "\n",
       "                                               tweet  \n",
       "0  RT @Jav_grandpa: [DASD-669] An Erotically Brai...  \n",
       "1  RT @gp_pulipaka: Best #Books That Will Teach Y...  \n",
       "2  RT @gp_pulipaka: Best #Books That Will Teach Y...  \n",
       "3  RT @CaptMarkKelly: I’m running for Senate to b...  \n",
       "4  #Microsoft is reducing dozens of MSN records p...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor = twitterdb.cursor()\n",
    "query = \"select created_at,username, tweet from twitterdb.twitterdata\"\n",
    "#cursor.execute(query)\n",
    "#data = cursor.fetchall()\n",
    "# store in dataframe\n",
    "df = pd.read_sql(query,twitterdb,columns=['time','username','tweet'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import mysql.connector \n",
    "from mysql.connector import Error\n",
    "import os\n",
    "import re\n",
    "import pandas as pd \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from textblob import TextBlob\n",
    "import pymysql\n",
    "\n",
    "class TweetObject():\n",
    "\t\n",
    "\tdef __init__(self, host, database, user):\n",
    "\t\tself.password = 'ck1234'\n",
    "\t\tself.host = host\n",
    "\t\tself.database = database\n",
    "\t\tself.user = user\n",
    "\t\t\n",
    "\n",
    "\n",
    "\tdef MySQLConnect(self,query):\n",
    "\t\t\"\"\"\n",
    "\t\tConnects to database and extracts\n",
    "\t\traw tweets and any other columns we\n",
    "\t\tneed\n",
    "\t\tParameters:\n",
    "\t\t----------------\n",
    "\t\targ1: string: SQL query\n",
    "\t\tReturns: pandas dataframr\n",
    "\t\t----------------\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\tcon = pymysql.connect(host = self.host, database = self.database, \\\n",
    "\t\t\t\tuser = self.user, password = self.password,charset = 'utf8')\n",
    "\n",
    "\t\t\tif con.open:\n",
    "\t\t\t\tprint(\"Successfully connected to database\")\n",
    "\t\t\t\tcursor = con.cursor()\n",
    "\t\t\t\tquery = query\n",
    "\t\t\t\tcursor.execute(query)\n",
    "\n",
    "\t\t\t\tresult_data = cursor.fetchall()\n",
    "\t\t\t\tdata = []\n",
    "\t\t\t\tfor row in result_data:\n",
    "\t\t\t\t\tdata.append({'date': row[\"created_at\"], 'tweet': row[\"tweet\"]})\n",
    "\t\t\t\t# store in dataframe\n",
    "\t\t\t\t#df = pd.DataFrame(data,columns = ['date', 'tweet'])\n",
    "\t\t\t\t#df = pd.read_sql_table(twitterdata,con,columns = ['date', 'tweet'])\n",
    "\t\t\t\t#print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "\t\texcept pymysql.Error as e:\n",
    "\t\t\tprint(e)\n",
    "\t\t\n",
    "\t\t#cursor.close()\n",
    "\t\tcon.close()\n",
    "\n",
    "\t\t\n",
    "\t\treturn df\n",
    "    \n",
    "\tdef clean_tweets(self, df):\n",
    "    \t\n",
    "    \t\t\"\"\"\n",
    "    \t\tTakes raw tweets and cleans them\n",
    "    \t\tso we can carry out analysis\n",
    "    \t\tremove stopwords, punctuation,\n",
    "    \t\tlower case, html, emoticons.\n",
    "    \t\tThis will be done using Regex\n",
    "    \t\t? means option so colou?r matches\n",
    "    \t\tboth color and colour.\n",
    "    \t\t\"\"\"\n",
    "    \n",
    "    \t\t# text preprocessing\n",
    "    \t\tstopword_list = stopwords.words('english')\n",
    "    \t\t#ps = PorterStemmer()\n",
    "    \t\twordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \t\tdf[\"clean_tweets\"] = None\n",
    "    \t\tdf['len'] = None\n",
    "    \t\tfor i in range(0,len(df['tweet'])):\n",
    "    \t\t\t# get rid of anything that isnt a letter\n",
    "    \n",
    "    \t\t\texclusion_list = ['[^a-zA-Z]','rt', 'http', 'co', 'RT']\n",
    "    \t\t\texclusions = '|'.join(exclusion_list)\n",
    "    \t\t\ttext = re.sub(exclusions, ' ' , df['tweet'][i])\n",
    "    \t\t\ttext = text.lower()\n",
    "    \t\t\twords = text.split()\n",
    "    \t\t\twords = [wordnet_lemmatizer.lemmatize(word) for word in words if not word in stopword_list]\n",
    "    \t\t\t # only use stem of word\n",
    "    \t\t\t#words = [ps.stem(word) for word in words]\n",
    "    \t\t\tdf['clean_tweets'][i] = ' '.join(words)\n",
    "    \n",
    "    \n",
    "    \t\t# Create column with data length\n",
    "    \t\tdf['len'] = np.array([len(tweet) for tweet in df[\"clean_tweets\"]])\n",
    "    \t\t\t\n",
    "    \n",
    "    \n",
    "    \t\treturn df    \n",
    "    \n",
    "\tdef sentiment(self, tweet):\n",
    "\t\t\"\"\"\n",
    "\t\tThis function calculates sentiment\n",
    "\t\ton our cleaned tweets.\n",
    "\t\tUses textblob to calculate polarity.\n",
    "\t\tParameters:\n",
    "\t\t----------------\n",
    "\t\targ1: takes in a tweet (row of dataframe)\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# need to improce\n",
    "\t\tanalysis = TextBlob(tweet)\n",
    "\t\tif analysis.sentiment.polarity > 0:\n",
    "\t\t\treturn 1\n",
    "\t\telif analysis.sentiment.polarity == 0:\n",
    "\t\t\treturn 0\n",
    "\t\telse:\n",
    "\t\t\treturn -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\tdef save_to_csv(self, df):\n",
    "\t\t\"\"\"\n",
    "\t\tSave cleaned data to a csv for further\n",
    "\t\tanalysis.\n",
    "\t\tParameters:\n",
    "\t\t----------------\n",
    "\t\targ1: Pandas dataframe\n",
    "\t\t\"\"\"\n",
    "\t\ttry:\n",
    "\t\t\tdf.to_csv(\"clean_tweets.csv\")\n",
    "\t\t\tprint(\"\\n\")\n",
    "\t\t\tprint(\"csv successfully saved. \\n\")\n",
    "\n",
    "\t\t\n",
    "\t\texcept Error as e:\n",
    "\t\t\tprint(e)\n",
    "\t\t\n",
    "\n",
    "\n",
    "\n",
    "\tdef word_cloud(self, df):\n",
    "\t\tplt.subplots(figsize = (12,10))\n",
    "\t\twordcloud = WordCloud(\n",
    "\t\t\t\tbackground_color = 'white',\n",
    "\t\t\t\twidth = 1000,\n",
    "\t\t\t\theight = 800).generate(\" \".join(df['clean_tweets']))\n",
    "\t\tplt.imshow(wordcloud)\n",
    "\t\tplt.axis('off')\n",
    "\t\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\tt = TweetObject( host = 'localhost', database = 'twitterdb', user = 'root')\n",
    "\n",
    "\tdata  = t.MySQLConnect(\"\"\"SELECT created_at, tweet FROM twitterdata\"\"\")\n",
    "\tdata = t.clean_tweets(data)\n",
    "\tdata['Sentiment'] = np.array([t.sentiment(x) for x in data['clean_tweets']])\n",
    "\tt.word_cloud(data)\n",
    "\tt.save_to_csv(data)\n",
    "\t\n",
    "\tpos_tweets = [tweet for index, tweet in enumerate(data[\"clean_tweets\"]) if data[\"Sentiment\"][index] > 0]\n",
    "\tneg_tweets = [tweet for index, tweet in enumerate(data[\"clean_tweets\"]) if data[\"Sentiment\"][index] < 0]\n",
    "\tneu_tweets = [tweet for index, tweet in enumerate(data[\"clean_tweets\"]) if data[\"Sentiment\"][index] == 0]\n",
    "\n",
    "\t#Print results\n",
    "\tprint(\"percentage of positive tweets: {}%\".format(100*(len(pos_tweets)/len(data['clean_tweets']))))\n",
    "\tprint(\"percentage of negative tweets: {}%\".format(100*(len(neg_tweets)/len(data['clean_tweets']))))\n",
    "\tprint(\"percentage of neutral tweets: {}%\".format(100*(len(neu_tweets)/len(data['clean_tweets']))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
